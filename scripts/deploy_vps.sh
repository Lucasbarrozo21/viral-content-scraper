#!/bin/bash

# =================================================================
# SCRIPT DE DEPLOY AUTOMATIZADO - SISTEMA VIRAL SCRAPER
# Sistema de Scraping Inteligente para Conte√∫do Viral
# 
# Este script automatiza completamente o deploy em VPS
# Autor: Manus AI
# Data: 27 de Janeiro de 2025
# Vers√£o: 2.0 - REVOLUTIONARY EDITION
# =================================================================

set -e  # Parar em caso de erro

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Fun√ß√£o para log colorido
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
    exit 1
}

warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

# Banner de in√≠cio
echo -e "${PURPLE}"
cat << "EOF"
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                              ‚ïë
‚ïë    üöÄ VIRAL CONTENT SCRAPER - DEPLOY AUTOMATIZADO üöÄ        ‚ïë
‚ïë                                                              ‚ïë
‚ïë    Sistema de Scraping Inteligente para Conte√∫do Viral      ‚ïë
‚ïë    Vers√£o 2.0 - REVOLUTIONARY EDITION                       ‚ïë
‚ïë                                                              ‚ïë
‚ïë    Desenvolvido por: Manus AI                                ‚ïë
‚ïë    Data: 27 de Janeiro de 2025                               ‚ïë
‚ïë                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
echo -e "${NC}"

# Verificar se est√° rodando como root
if [[ $EUID -eq 0 ]]; then
   error "Este script n√£o deve ser executado como root. Use um usu√°rio com sudo."
fi

# Verificar sistema operacional
if [[ ! -f /etc/os-release ]]; then
    error "Sistema operacional n√£o suportado. Use Ubuntu 20.04+ ou Debian 10+"
fi

source /etc/os-release
if [[ $ID != "ubuntu" && $ID != "debian" ]]; then
    error "Sistema operacional n√£o suportado: $ID. Use Ubuntu ou Debian."
fi

log "üéØ Iniciando deploy do Sistema Viral Scraper..."
log "üìä Sistema detectado: $PRETTY_NAME"

# Configura√ß√µes
PROJECT_DIR="/opt/viral_scraper"
BACKUP_DIR="/opt/backups/viral_scraper"
LOG_DIR="/var/log/viral_scraper"
NGINX_SITES="/etc/nginx/sites-available"
SYSTEMD_DIR="/etc/systemd/system"

# Solicitar informa√ß√µes do usu√°rio
echo -e "\n${CYAN}üìù CONFIGURA√á√ÉO INICIAL${NC}"
echo "Por favor, forne√ßa as seguintes informa√ß√µes:"

read -p "üåê Dom√≠nio (ex: scraper.exemplo.com): " DOMAIN
read -p "üìß Email para SSL (Let's Encrypt): " EMAIL
read -s -p "üîê Senha do banco PostgreSQL: " DB_PASSWORD
echo
read -s -p "üîë OpenAI API Key: " OPENAI_API_KEY
echo
read -p "üì± Webhook Slack (opcional): " SLACK_WEBHOOK

# Validar inputs obrigat√≥rios
if [[ -z "$DOMAIN" || -z "$EMAIL" || -z "$DB_PASSWORD" || -z "$OPENAI_API_KEY" ]]; then
    error "Todos os campos obrigat√≥rios devem ser preenchidos!"
fi

log "‚úÖ Configura√ß√£o inicial conclu√≠da"

# Fun√ß√£o para instalar depend√™ncias
install_dependencies() {
    log "üì¶ Instalando depend√™ncias do sistema..."
    
    # Atualizar sistema
    sudo apt update && sudo apt upgrade -y
    
    # Instalar depend√™ncias b√°sicas
    sudo apt install -y \
        curl \
        wget \
        git \
        htop \
        unzip \
        software-properties-common \
        apt-transport-https \
        ca-certificates \
        gnupg \
        lsb-release \
        build-essential
    
    log "‚úÖ Depend√™ncias b√°sicas instaladas"
}

# Fun√ß√£o para instalar Node.js
install_nodejs() {
    log "üü¢ Instalando Node.js 18..."
    
    # Remover vers√µes antigas
    sudo apt remove -y nodejs npm
    
    # Instalar Node.js 18 via NodeSource
    curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
    sudo apt install -y nodejs
    
    # Verificar instala√ß√£o
    node_version=$(node --version)
    npm_version=$(npm --version)
    
    log "‚úÖ Node.js instalado: $node_version"
    log "‚úÖ NPM instalado: $npm_version"
}

# Fun√ß√£o para instalar Python
install_python() {
    log "üêç Instalando Python 3.9+..."
    
    sudo apt install -y \
        python3 \
        python3-pip \
        python3-venv \
        python3-dev
    
    # Atualizar pip
    python3 -m pip install --upgrade pip
    
    python_version=$(python3 --version)
    pip_version=$(python3 -m pip --version)
    
    log "‚úÖ Python instalado: $python_version"
    log "‚úÖ Pip instalado: $pip_version"
}

# Fun√ß√£o para instalar PostgreSQL
install_postgresql() {
    log "üêò Instalando PostgreSQL..."
    
    sudo apt install -y postgresql postgresql-contrib
    
    # Iniciar e habilitar servi√ßo
    sudo systemctl start postgresql
    sudo systemctl enable postgresql
    
    # Criar usu√°rio e banco
    sudo -u postgres psql << EOF
CREATE USER viral_user WITH PASSWORD '$DB_PASSWORD';
CREATE DATABASE viral_content_db OWNER viral_user;
GRANT ALL PRIVILEGES ON DATABASE viral_content_db TO viral_user;
ALTER USER viral_user CREATEDB;
\q
EOF
    
    log "‚úÖ PostgreSQL instalado e configurado"
}

# Fun√ß√£o para instalar Redis
install_redis() {
    log "üî¥ Instalando Redis..."
    
    sudo apt install -y redis-server
    
    # Configurar Redis
    sudo sed -i 's/supervised no/supervised systemd/' /etc/redis/redis.conf
    
    # Iniciar e habilitar servi√ßo
    sudo systemctl start redis-server
    sudo systemctl enable redis-server
    
    log "‚úÖ Redis instalado e configurado"
}

# Fun√ß√£o para instalar Nginx
install_nginx() {
    log "üåê Instalando Nginx..."
    
    sudo apt install -y nginx
    
    # Iniciar e habilitar servi√ßo
    sudo systemctl start nginx
    sudo systemctl enable nginx
    
    log "‚úÖ Nginx instalado"
}

# Fun√ß√£o para instalar Chromium (para Puppeteer)
install_chromium() {
    log "üåê Instalando Chromium para Puppeteer..."
    
    sudo apt install -y \
        chromium-browser \
        fonts-liberation \
        libasound2 \
        libatk-bridge2.0-0 \
        libdrm2 \
        libgtk-3-0 \
        libgtk-4-1 \
        libnspr4 \
        libnss3 \
        libxcomposite1 \
        libxdamage1 \
        libxrandr2 \
        xdg-utils
    
    log "‚úÖ Chromium instalado"
}

# Fun√ß√£o para clonar reposit√≥rio
clone_repository() {
    log "üì• Clonando reposit√≥rio do projeto..."
    
    # Criar diret√≥rio do projeto
    sudo mkdir -p $PROJECT_DIR
    sudo chown -R $USER:$USER $PROJECT_DIR
    
    # Clonar reposit√≥rio (assumindo que j√° existe localmente)
    if [[ -d "/home/ubuntu/viral_content_scraper" ]]; then
        cp -r /home/ubuntu/viral_content_scraper/* $PROJECT_DIR/
        log "‚úÖ Projeto copiado de /home/ubuntu/viral_content_scraper"
    else
        error "Diret√≥rio do projeto n√£o encontrado em /home/ubuntu/viral_content_scraper"
    fi
    
    # Definir permiss√µes
    sudo chown -R $USER:$USER $PROJECT_DIR
    chmod +x $PROJECT_DIR/scripts/*.sh
}

# Fun√ß√£o para instalar depend√™ncias do projeto
install_project_dependencies() {
    log "üì¶ Instalando depend√™ncias do projeto..."
    
    cd $PROJECT_DIR
    
    # Scrapers (Node.js)
    log "üï∑Ô∏è Instalando depend√™ncias dos scrapers..."
    cd scrapers
    npm install --production
    
    # AI Agents (Node.js)
    log "ü§ñ Instalando depend√™ncias dos agentes IA..."
    cd ../ai_agents
    npm install --production
    
    # API (Python)
    log "üîå Instalando depend√™ncias da API..."
    cd ../api
    python3 -m pip install -r requirements.txt
    
    # Frontend (Node.js)
    log "üíª Instalando depend√™ncias do frontend..."
    cd ../viral-dashboard
    npm install
    npm run build
    
    log "‚úÖ Todas as depend√™ncias instaladas"
}

# Fun√ß√£o para configurar vari√°veis de ambiente
configure_environment() {
    log "‚öôÔ∏è Configurando vari√°veis de ambiente..."
    
    cd $PROJECT_DIR
    
    # Criar arquivo .env
    cat > config/.env << EOF
# Database Configuration
DATABASE_URL=postgresql://viral_user:$DB_PASSWORD@localhost:5432/viral_content_db
REDIS_URL=redis://localhost:6379/0

# OpenAI Configuration
OPENAI_API_KEY=$OPENAI_API_KEY
OPENAI_API_BASE=https://api.openai.com/v1

# Flask Configuration
FLASK_SECRET_KEY=$(openssl rand -hex 32)
JWT_SECRET_KEY=$(openssl rand -hex 32)
FLASK_ENV=production

# Security
ALLOWED_HOSTS=$DOMAIN,localhost,127.0.0.1
CORS_ORIGINS=https://$DOMAIN,http://localhost:3000

# Logging
LOG_LEVEL=INFO
LOG_DIR=$LOG_DIR

# Monitoring
SLACK_WEBHOOK_URL=$SLACK_WEBHOOK

# Scrapers Configuration
HEADLESS=true
RATE_LIMIT_ENABLED=true
MAX_CONCURRENT_SCRAPERS=3
SCRAPER_TIMEOUT=60000

# Cache Configuration
CACHE_TTL=3600
REDIS_MAX_CONNECTIONS=10

# File Storage
UPLOAD_FOLDER=$PROJECT_DIR/uploads
MAX_CONTENT_LENGTH=16777216

# Performance
WORKERS=4
THREADS=2
TIMEOUT=30
EOF
    
    # Definir permiss√µes seguras
    chmod 600 config/.env
    
    log "‚úÖ Vari√°veis de ambiente configuradas"
}

# Fun√ß√£o para executar migrations do banco
run_migrations() {
    log "üóÑÔ∏è Executando migrations do banco de dados..."
    
    cd $PROJECT_DIR/database
    
    # Executar schema principal
    PGPASSWORD=$DB_PASSWORD psql -h localhost -U viral_user -d viral_content_db -f create_schema.sql
    
    # Executar migrations se existirem
    if [[ -d "migrations" ]]; then
        for migration in migrations/*.sql; do
            if [[ -f "$migration" ]]; then
                log "üìÑ Executando migration: $(basename $migration)"
                PGPASSWORD=$DB_PASSWORD psql -h localhost -U viral_user -d viral_content_db -f "$migration"
            fi
        done
    fi
    
    log "‚úÖ Migrations executadas com sucesso"
}

# Fun√ß√£o para criar diret√≥rios necess√°rios
create_directories() {
    log "üìÅ Criando diret√≥rios necess√°rios..."
    
    # Diret√≥rios de log
    sudo mkdir -p $LOG_DIR
    sudo chown -R $USER:$USER $LOG_DIR
    
    # Diret√≥rio de backup
    sudo mkdir -p $BACKUP_DIR
    sudo chown -R $USER:$USER $BACKUP_DIR
    
    # Diret√≥rio de uploads
    mkdir -p $PROJECT_DIR/uploads
    mkdir -p $PROJECT_DIR/temp
    
    log "‚úÖ Diret√≥rios criados"
}

# Fun√ß√£o para configurar servi√ßos systemd
configure_systemd_services() {
    log "üîß Configurando servi√ßos systemd..."
    
    # Servi√ßo da API
    sudo tee $SYSTEMD_DIR/viral-api.service > /dev/null << EOF
[Unit]
Description=Viral Scraper API
After=network.target postgresql.service redis.service
Requires=postgresql.service redis.service

[Service]
Type=simple
User=$USER
Group=$USER
WorkingDirectory=$PROJECT_DIR/api
Environment=PATH=/usr/bin:/usr/local/bin
EnvironmentFile=$PROJECT_DIR/config/.env
ExecStart=/usr/bin/python3 app_simple.py
Restart=always
RestartSec=10
StandardOutput=append:$LOG_DIR/api.log
StandardError=append:$LOG_DIR/api-error.log

# Security
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=$PROJECT_DIR $LOG_DIR

[Install]
WantedBy=multi-user.target
EOF

    # Servi√ßo dos Scrapers
    sudo tee $SYSTEMD_DIR/viral-scrapers.service > /dev/null << EOF
[Unit]
Description=Viral Content Scrapers
After=network.target redis.service viral-api.service
Requires=redis.service

[Service]
Type=simple
User=$USER
Group=$USER
WorkingDirectory=$PROJECT_DIR/scrapers
Environment=PATH=/usr/bin:/usr/local/bin
Environment=NODE_ENV=production
EnvironmentFile=$PROJECT_DIR/config/.env
ExecStart=/usr/bin/node src/index.js
Restart=always
RestartSec=30
StandardOutput=append:$LOG_DIR/scrapers.log
StandardError=append:$LOG_DIR/scrapers-error.log

# Security
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=$PROJECT_DIR $LOG_DIR

[Install]
WantedBy=multi-user.target
EOF

    # Servi√ßo dos Agentes IA
    sudo tee $SYSTEMD_DIR/viral-agents.service > /dev/null << EOF
[Unit]
Description=Viral AI Agents
After=network.target redis.service viral-api.service
Requires=redis.service

[Service]
Type=simple
User=$USER
Group=$USER
WorkingDirectory=$PROJECT_DIR/ai_agents
Environment=PATH=/usr/bin:/usr/local/bin
Environment=NODE_ENV=production
EnvironmentFile=$PROJECT_DIR/config/.env
ExecStart=/usr/bin/node src/index.js
Restart=always
RestartSec=30
StandardOutput=append:$LOG_DIR/agents.log
StandardError=append:$LOG_DIR/agents-error.log

# Security
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=$PROJECT_DIR $LOG_DIR

[Install]
WantedBy=multi-user.target
EOF

    # Recarregar systemd
    sudo systemctl daemon-reload
    
    log "‚úÖ Servi√ßos systemd configurados"
}

# Fun√ß√£o para configurar Nginx
configure_nginx() {
    log "üåê Configurando Nginx..."
    
    # Configura√ß√£o do site
    sudo tee $NGINX_SITES/viral-scraper.conf > /dev/null << EOF
# Viral Scraper Nginx Configuration
server {
    listen 80;
    server_name $DOMAIN;
    
    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header Referrer-Policy "no-referrer-when-downgrade" always;
    add_header Content-Security-Policy "default-src 'self' http: https: data: blob: 'unsafe-inline' 'unsafe-eval'" always;
    
    # Rate limiting
    limit_req_zone \$binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone \$binary_remote_addr zone=login:10m rate=1r/s;
    
    # Frontend (React build)
    location / {
        root $PROJECT_DIR/viral-dashboard/dist;
        try_files \$uri \$uri/ /index.html;
        
        # Cache static assets
        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)\$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }
    
    # API endpoints
    location /api/ {
        limit_req zone=api burst=20 nodelay;
        
        proxy_pass http://127.0.0.1:5000;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;
        
        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # Buffer settings
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
    }
    
    # Login endpoint with stricter rate limiting
    location /api/v1/auth/login {
        limit_req zone=login burst=5 nodelay;
        
        proxy_pass http://127.0.0.1:5000;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;
    }
    
    # Health check
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
    
    # Block access to sensitive files
    location ~ /\. {
        deny all;
        access_log off;
        log_not_found off;
    }
    
    location ~ \.(env|log|sql|md)\$ {
        deny all;
        access_log off;
        log_not_found off;
    }
}
EOF
    
    # Habilitar site
    sudo ln -sf $NGINX_SITES/viral-scraper.conf /etc/nginx/sites-enabled/
    
    # Remover site padr√£o
    sudo rm -f /etc/nginx/sites-enabled/default
    
    # Testar configura√ß√£o
    sudo nginx -t
    
    # Recarregar Nginx
    sudo systemctl reload nginx
    
    log "‚úÖ Nginx configurado"
}

# Fun√ß√£o para configurar SSL com Let's Encrypt
configure_ssl() {
    log "üîí Configurando SSL com Let's Encrypt..."
    
    # Instalar Certbot
    sudo apt install -y certbot python3-certbot-nginx
    
    # Obter certificado SSL
    sudo certbot --nginx -d $DOMAIN --email $EMAIL --agree-tos --non-interactive --redirect
    
    # Configurar renova√ß√£o autom√°tica
    sudo systemctl enable certbot.timer
    
    log "‚úÖ SSL configurado com sucesso"
}

# Fun√ß√£o para configurar logrotate
configure_logrotate() {
    log "üìù Configurando rota√ß√£o de logs..."
    
    sudo tee /etc/logrotate.d/viral-scraper > /dev/null << EOF
$LOG_DIR/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    create 644 $USER $USER
    postrotate
        systemctl reload viral-api viral-scrapers viral-agents
    endscript
}
EOF
    
    log "‚úÖ Logrotate configurado"
}

# Fun√ß√£o para configurar backup autom√°tico
configure_backup() {
    log "üíæ Configurando backup autom√°tico..."
    
    # Script de backup
    tee $PROJECT_DIR/scripts/backup.sh > /dev/null << EOF
#!/bin/bash
# Backup autom√°tico do Viral Scraper

BACKUP_DIR="$BACKUP_DIR"
DATE=\$(date +%Y%m%d_%H%M%S)
LOG_FILE="$LOG_DIR/backup.log"

echo "[\$(date)] Iniciando backup..." >> \$LOG_FILE

# Criar diret√≥rio de backup
mkdir -p \$BACKUP_DIR

# Backup do banco de dados
PGPASSWORD=$DB_PASSWORD pg_dump -h localhost -U viral_user viral_content_db > \$BACKUP_DIR/database_\$DATE.sql
echo "[\$(date)] Backup do banco conclu√≠do" >> \$LOG_FILE

# Backup das configura√ß√µes
tar -czf \$BACKUP_DIR/config_\$DATE.tar.gz -C $PROJECT_DIR config/
echo "[\$(date)] Backup das configura√ß√µes conclu√≠do" >> \$LOG_FILE

# Backup dos logs importantes
tar -czf \$BACKUP_DIR/logs_\$DATE.tar.gz -C $LOG_DIR .
echo "[\$(date)] Backup dos logs conclu√≠do" >> \$LOG_FILE

# Limpar backups antigos (manter 30 dias)
find \$BACKUP_DIR -name "*.sql" -mtime +30 -delete
find \$BACKUP_DIR -name "*.tar.gz" -mtime +30 -delete
echo "[\$(date)] Limpeza de backups antigos conclu√≠da" >> \$LOG_FILE

echo "[\$(date)] Backup conclu√≠do com sucesso" >> \$LOG_FILE
EOF
    
    chmod +x $PROJECT_DIR/scripts/backup.sh
    
    # Adicionar ao crontab
    (crontab -l 2>/dev/null; echo "0 2 * * * $PROJECT_DIR/scripts/backup.sh") | crontab -
    
    log "‚úÖ Backup autom√°tico configurado (di√°rio √†s 2h)"
}

# Fun√ß√£o para configurar monitoramento
configure_monitoring() {
    log "üìä Configurando monitoramento..."
    
    # Script de monitoramento
    tee $PROJECT_DIR/scripts/monitor.sh > /dev/null << EOF
#!/bin/bash
# Monitor do sistema Viral Scraper

LOG_FILE="$LOG_DIR/monitor.log"
ALERT_THRESHOLD_CPU=80
ALERT_THRESHOLD_MEM=85
ALERT_THRESHOLD_DISK=90

# Fun√ß√£o para enviar alerta
send_alert() {
    local message="\$1"
    local severity="\$2"
    
    echo "[\$(date)] ALERT [\$severity]: \$message" >> \$LOG_FILE
    
    # Enviar para Slack se configurado
    if [[ -n "$SLACK_WEBHOOK" ]]; then
        curl -X POST -H 'Content-type: application/json' \\
            --data "{\\"text\\":\\"üö® Viral Scraper Alert: \$message\\"}" \\
            "$SLACK_WEBHOOK" 2>/dev/null
    fi
}

# Verificar servi√ßos
for service in viral-api viral-scrapers viral-agents postgresql redis nginx; do
    if ! systemctl is-active --quiet \$service; then
        send_alert "Servi√ßo \$service est√° inativo" "CRITICAL"
    fi
done

# Verificar uso de CPU
CPU_USAGE=\$(top -bn1 | grep "Cpu(s)" | awk '{print \$2}' | awk -F'%' '{print \$1}')
if (( \$(echo "\$CPU_USAGE > \$ALERT_THRESHOLD_CPU" | bc -l) )); then
    send_alert "Alto uso de CPU: \${CPU_USAGE}%" "WARNING"
fi

# Verificar uso de mem√≥ria
MEM_USAGE=\$(free | awk 'NR==2{printf "%.1f", \$3*100/\$2 }')
if (( \$(echo "\$MEM_USAGE > \$ALERT_THRESHOLD_MEM" | bc -l) )); then
    send_alert "Alto uso de mem√≥ria: \${MEM_USAGE}%" "WARNING"
fi

# Verificar uso de disco
DISK_USAGE=\$(df / | awk 'NR==2{print \$5}' | sed 's/%//')
if (( DISK_USAGE > ALERT_THRESHOLD_DISK )); then
    send_alert "Alto uso de disco: \${DISK_USAGE}%" "WARNING"
fi

# Verificar conectividade da API
if ! curl -f -s http://localhost:5000/health > /dev/null; then
    send_alert "API n√£o est√° respondendo" "CRITICAL"
fi

echo "[\$(date)] Monitoramento executado" >> \$LOG_FILE
EOF
    
    chmod +x $PROJECT_DIR/scripts/monitor.sh
    
    # Adicionar ao crontab (executar a cada 5 minutos)
    (crontab -l 2>/dev/null; echo "*/5 * * * * $PROJECT_DIR/scripts/monitor.sh") | crontab -
    
    log "‚úÖ Monitoramento configurado (execu√ß√£o a cada 5 minutos)"
}

# Fun√ß√£o para iniciar servi√ßos
start_services() {
    log "üöÄ Iniciando servi√ßos..."
    
    # Habilitar servi√ßos para iniciar no boot
    sudo systemctl enable viral-api viral-scrapers viral-agents
    
    # Iniciar servi√ßos
    sudo systemctl start viral-api
    sleep 5
    sudo systemctl start viral-scrapers
    sleep 5
    sudo systemctl start viral-agents
    
    # Verificar status
    for service in viral-api viral-scrapers viral-agents; do
        if systemctl is-active --quiet $service; then
            log "‚úÖ Servi√ßo $service iniciado com sucesso"
        else
            error "‚ùå Falha ao iniciar servi√ßo $service"
        fi
    done
}

# Fun√ß√£o para verificar instala√ß√£o
verify_installation() {
    log "üîç Verificando instala√ß√£o..."
    
    # Verificar servi√ßos
    echo -e "\n${CYAN}üìä STATUS DOS SERVI√áOS:${NC}"
    for service in postgresql redis nginx viral-api viral-scrapers viral-agents; do
        if systemctl is-active --quiet $service; then
            echo -e "  ‚úÖ $service: ${GREEN}Ativo${NC}"
        else
            echo -e "  ‚ùå $service: ${RED}Inativo${NC}"
        fi
    done
    
    # Verificar conectividade
    echo -e "\n${CYAN}üåê CONECTIVIDADE:${NC}"
    if curl -f -s http://localhost:5000/health > /dev/null; then
        echo -e "  ‚úÖ API Local: ${GREEN}OK${NC}"
    else
        echo -e "  ‚ùå API Local: ${RED}Falha${NC}"
    fi
    
    if curl -f -s https://$DOMAIN/health > /dev/null; then
        echo -e "  ‚úÖ HTTPS: ${GREEN}OK${NC}"
    else
        echo -e "  ‚ùå HTTPS: ${RED}Falha${NC}"
    fi
    
    # Verificar recursos
    echo -e "\n${CYAN}üíª RECURSOS DO SISTEMA:${NC}"
    echo -e "  üìä CPU: $(nproc) cores"
    echo -e "  üß† RAM: $(free -h | awk 'NR==2{print $2}') total, $(free -h | awk 'NR==2{print $7}') dispon√≠vel"
    echo -e "  üíæ Disco: $(df -h / | awk 'NR==2{print $2}') total, $(df -h / | awk 'NR==2{print $4}') dispon√≠vel"
    
    # Verificar logs
    echo -e "\n${CYAN}üìù LOGS RECENTES:${NC}"
    if [[ -f "$LOG_DIR/api.log" ]]; then
        echo -e "  üìÑ API (√∫ltimas 3 linhas):"
        tail -n 3 $LOG_DIR/api.log | sed 's/^/    /'
    fi
}

# Fun√ß√£o para mostrar informa√ß√µes finais
show_final_info() {
    echo -e "\n${GREEN}"
    cat << "EOF"
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                              ‚ïë
‚ïë    üéâ DEPLOY CONCLU√çDO COM SUCESSO! üéâ                      ‚ïë
‚ïë                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
EOF
    echo -e "${NC}"
    
    echo -e "${CYAN}üìã INFORMA√á√ïES DO SISTEMA:${NC}"
    echo -e "  üåê URL: ${GREEN}https://$DOMAIN${NC}"
    echo -e "  üìä Dashboard: ${GREEN}https://$DOMAIN${NC}"
    echo -e "  üîå API: ${GREEN}https://$DOMAIN/api/v1${NC}"
    echo -e "  üìÅ Projeto: ${GREEN}$PROJECT_DIR${NC}"
    echo -e "  üìù Logs: ${GREEN}$LOG_DIR${NC}"
    echo -e "  üíæ Backups: ${GREEN}$BACKUP_DIR${NC}"
    
    echo -e "\n${CYAN}üîß COMANDOS √öTEIS:${NC}"
    echo -e "  üìä Status dos servi√ßos: ${YELLOW}sudo systemctl status viral-api viral-scrapers viral-agents${NC}"
    echo -e "  üìù Ver logs: ${YELLOW}tail -f $LOG_DIR/api.log${NC}"
    echo -e "  üîÑ Reiniciar API: ${YELLOW}sudo systemctl restart viral-api${NC}"
    echo -e "  üìä Monitor: ${YELLOW}$PROJECT_DIR/scripts/monitor.sh${NC}"
    echo -e "  üíæ Backup: ${YELLOW}$PROJECT_DIR/scripts/backup.sh${NC}"
    
    echo -e "\n${CYAN}üìö DOCUMENTA√á√ÉO:${NC}"
    echo -e "  üìñ Documenta√ß√£o completa: ${GREEN}$PROJECT_DIR/docs/SISTEMA_COMPLETO_DOCUMENTACAO.md${NC}"
    echo -e "  üîß Configura√ß√£o: ${GREEN}$PROJECT_DIR/config/.env${NC}"
    
    echo -e "\n${CYAN}üö® PR√ìXIMOS PASSOS:${NC}"
    echo -e "  1. Acesse ${GREEN}https://$DOMAIN${NC} para verificar o dashboard"
    echo -e "  2. Configure as integra√ß√µes necess√°rias"
    echo -e "  3. Execute os primeiros scrapers de teste"
    echo -e "  4. Configure alertas e monitoramento adicional"
    echo -e "  5. Fa√ßa backup das configura√ß√µes"
    
    echo -e "\n${PURPLE}üéØ SISTEMA VIRAL SCRAPER EST√Å PRONTO PARA GERAR BILH√ïES!${NC}"
}

# Fun√ß√£o principal
main() {
    log "üöÄ Iniciando processo de deploy..."
    
    # Executar todas as etapas
    install_dependencies
    install_nodejs
    install_python
    install_postgresql
    install_redis
    install_nginx
    install_chromium
    clone_repository
    install_project_dependencies
    configure_environment
    create_directories
    run_migrations
    configure_systemd_services
    configure_nginx
    configure_ssl
    configure_logrotate
    configure_backup
    configure_monitoring
    start_services
    
    # Aguardar servi√ßos iniciarem
    log "‚è≥ Aguardando servi√ßos estabilizarem..."
    sleep 10
    
    # Verificar instala√ß√£o
    verify_installation
    
    # Mostrar informa√ß√µes finais
    show_final_info
    
    log "üéâ Deploy conclu√≠do com sucesso!"
}

# Verificar se o usu√°rio quer continuar
echo -e "\n${YELLOW}‚ö†Ô∏è  ATEN√á√ÉO:${NC}"
echo "Este script ir√°:"
echo "  ‚Ä¢ Instalar e configurar todos os componentes necess√°rios"
echo "  ‚Ä¢ Modificar configura√ß√µes do sistema"
echo "  ‚Ä¢ Criar usu√°rios e bancos de dados"
echo "  ‚Ä¢ Configurar servi√ßos systemd"
echo "  ‚Ä¢ Obter certificados SSL"
echo ""
read -p "Deseja continuar? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Deploy cancelado pelo usu√°rio."
    exit 0
fi

# Executar deploy
main

exit 0

